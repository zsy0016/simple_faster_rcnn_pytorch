from util.bbox import *
from torchvision.ops import nms
import numpy as np


class RoICreator:
    """RoIs are generated by calling this object.

    The :meth:`__call__` of this object outputs object detection rois by applying estimated 
    bbox offsets to a set of anchors.

    This class takes parameters to control number of bboxes to pass to NMS and keep after NMS.
    If the paramters are negative, it uses all the bboxes supplied or keep all the bboxes 
    returned by NMS.

    This class is used for Region Proposal Networks introduced in
    Faster R-CNN [#]_.

    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. 
    Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. NIPS 2015.

    Args:
        parent_model (nn.Module): If :obj:'parent_model.training=True', use training parameters. 
            If :obj:'parent_model.training=False', use testing parameters.
        nms_thresh (float): Threshold value used when calling NMS.
        n_train_pre_nms (int): Number of top scored bboxes
            to keep before passing to NMS in train mode.
        n_train_post_nms (int): Number of top scored bboxes
            to keep after passing to NMS in train mode.
        n_test_pre_nms (int): Number of top scored bboxes
            to keep before passing to NMS in test mode.
        n_test_post_nms (int): Number of top scored bboxes
            to keep after passing to NMS in test mode.
        min_size (int): A paramter to determine the threshold on
            discarding bboxes based on their sizes.
    """
    def __init__(self, parent_model, nms_thresh=0.7, n_train_pre_nms=12000, n_train_post_nms=2000,
        n_test_pre_nms=6000, n_test_post_nms=300, min_size=16):
        self.parent_model     = parent_model
        self.nms_thresh       = nms_thresh
        self.n_train_pre_nms  = n_train_pre_nms
        self.n_train_post_nms = n_train_post_nms
        self.n_test_pre_nms   = n_test_pre_nms
        self.n_test_post_nms  = n_test_post_nms
        self.min_size         = min_size

    def __call__(self, anchor, loc, score, img_size, scale=1.):
        """Propose RoIs.

        Inputs :obj:`anchor, loc, score` refer to the same anchor when indexed by the same index.

        On notations, :math:`R` is the total number of anchors. This is equal to product of the 
        height and the width of an image and the number of anchor bases per pixel.

        Type of the output is same as the inputs.

        Args:
            anchor (torch.Tensor): Coordinates of anchors. Its shape is :math:`(R, 4)`.
            loc (torch.Tensor): Predicted offsets and scaling of anchors. 
                Its shape is :math:`(R, 4)`.
            score (torch.Tensor): Predicted foreground probability for anchors. 
                Its shape is :math:`(R,)`.
            img_size (tuple of ints): A tuple :obj:`height, width`, which contains 
                image size after scaling.
            scale (float): The scaling factor used to scale an image after
                reading it from a file.

        Returns:
            torch.Tensor:
            An array of coordinates of proposal boxes.
            Its shape is :math:`(S, 4)`. :math:`S` is less than
            :obj:`self.n_test_post_nms` in test time and less than
            :obj:`self.n_train_post_nms` in train time. :math:`S` depends on
            the size of the predicted bboxes and the number of
            bboxes discarded by NMS.
        """
        if self.parent_model.training:
            n_pre_nms  = self.n_train_pre_nms
            n_post_nms = self.n_train_post_nms
        else:
            n_pre_nms  = self.n_test_pre_nms
            n_post_nms = self.n_test_post_nms

        # Convert anchors into proposal via bbox transformations.
        roi_ = loc2bbox(anchor, loc)
        
        roi = torch.zeros_like(roi_).float().to(roi_.device)
        roi[:, 0::2] = torch.clamp(roi_[:, 0::2], 0, img_size[0])
        roi[:, 1::2] = torch.clamp(roi_[:, 1::2], 0, img_size[1])

        
        # Remove predicted boxes with either height or width < threshold.
        min_size = self.min_size * scale
        height   = roi[:, 2] - roi[:, 0]
        width    = roi[:, 3] - roi[:, 1]
        keep_index = torch.where((height >= min_size) & (width >= min_size))[0]
        roi        = roi[keep_index, :]
        score      = score[keep_index]

        # Sort all (roi, score) pairs by score descendingly and take top pre_nms_topN.
        order = score.argsort(descending=True)
        order = order[:n_pre_nms] if n_pre_nms > 0 else order
        roi   = roi[order, :]
        score = score[order]

        # Apply nms (e.g. threshold = 0.7) and after_nms_topN (e.g. 300).
        keep_index = nms(roi, score, self.nms_thresh)
        keep_index = keep_index[:n_post_nms] if n_post_nms > 0 else keep_index
        roi = roi[keep_index]
        return roi


class AnchorTargetCreator:
    """Assign the ground truth bboxes to anchors.

    Assigns the ground truth bboxes to anchors for training Region
    Proposal Networks introduced in Faster R-CNN [#]_.

    Offsets and scales to match anchors to the ground truth are calculated using the encoding 
    scheme of :func:`model.bbox.bbox2loc`.

    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. 
    Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. NIPS 2015.

    Args:
        n_sample (int): The number of regions to produce.
        pos_iou_thresh (float): Anchors with IoU above this threshold are assigned as positive.
        neg_iou_thresh (float): Anchors with IoU below this threshold are assigned as negative.
        pos_ratio (float): Ratio of positive regions in the sampled regions.
    """
    def __init__(self, n_sample=256, pos_iou_thresh=0.7, neg_iou_thresh=0.3, pos_ratio=0.5):
        self.n_sample = n_sample
        self.pos_iou_thresh = pos_iou_thresh
        self.neg_iou_thresh = neg_iou_thresh
        self.pos_ratio = pos_ratio

    def __call__(self, anchor, bbox, img_size):
        """Assign ground truth supervision to sampled subset of anchors.

        Types of input arrays and output arrays are same.

        Here are notations.

        * :math:`S` is the number of anchors.
        * :math:`R` is the number of bboxes.

        Args:
            anchor (torch.Tensor): Coordinates of anchors. Its shape is :math:`(S, 4)`.
            bbox (torch.Tensor): Coordinates of bboxes. Its shape is :math:`(R, 4)`.
            img_size (tuple of ints): A tuple :obj:`H, W`, which is a tuple of height.

        Returns:
            (torch.Tensor, torch.Tensor):
            * **loc**: Offsets and scales to match the anchors to the ground truth bboxes. 
                Its shape is :math:`(S, 4)`.
            * **label**: Labels of anchors with values :obj:`(1=positive, 0=negative, -1=ignore)`. 
                Its shape is :math:`(S,)`.
        """
        H, W = img_size

        n_anchor = len(anchor)
        inside_index = torch.where(
            (anchor[:, 0] >= 0) & 
            (anchor[:, 1] >= 0) & 
            (anchor[:, 2] <= H) &
            (anchor[:, 3] <= W))[0]
        anchor = anchor[inside_index]

        label = -1 * torch.ones((len(inside_index), )).long().to(anchor.device)
        iou = bbox_iou(anchor, bbox)
        anchor_max_iou = iou.max(dim=1)[0]
        label[torch.where(anchor_max_iou < self.neg_iou_thresh)[0]] = 0
        label[torch.where(anchor_max_iou >= self.pos_iou_thresh)[0]] = 1
        for i in range(len(bbox)):
            label[iou[:, i] == iou[:, i].max()] = 1

        # Positive
        pos_index = torch.where(label == 1)[0]
        n_pos_sample = int(self.pos_ratio * self.n_sample)
        n_pos_sample = int(min(n_pos_sample, pos_index.shape[0]))
        if len(pos_index) > n_pos_sample:
            disable_index = pos_index[np.random.choice(range(len(pos_index)), 
                len(pos_index) - n_pos_sample, replace=False)]
            label[disable_index] = -1
            pos_index = torch.where(label == 1)[0]

        # Negative
        n_neg_sample = self.n_sample - n_pos_sample
        neg_index = torch.where(label == 0)[0]
        if len(neg_index) > n_neg_sample:
            disable_index = neg_index[np.random.choice(range(len(neg_index)), 
                len(neg_index) - n_neg_sample, replace=False)]
            label[disable_index] = -1
            neg_index = torch.where(label == 0)[0]
        
        # Sample
        sample_index  = torch.cat([pos_index, neg_index], dim=0)
        sample_anchor = anchor[sample_index]
        sample_bbox   = bbox[iou.argmax(dim=1)][sample_index]
        sample_label  = label[sample_index]
        sample_loc    = bbox2loc(sample_anchor, sample_bbox)

        # Recover to original shape
        out_anchor = torch.zeros((n_anchor, 4)).float().to(anchor.device)
        out_loc    = torch.zeros((n_anchor, 4)).float().to(anchor.device)
        out_label  = -1 * torch.ones((n_anchor, )).long().to(anchor.device)
        out_anchor[inside_index[sample_index]] = sample_anchor
        out_loc[inside_index[sample_index]]    = sample_loc
        out_label[inside_index[sample_index]]  = sample_label

        return out_anchor, out_loc, out_label


class RoITargetCreator:
    """Assign ground truth bboxes to given RoIs.

    The :meth:`__call__` of this class generates training targets for each object proposal.
    This is used to train Faster RCNN [#]_.

    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. 
    Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. NIPS 2015.

    Args:
        n_sample (int): The number of sampled regions.
        pos_ratio (float): Fraction of regions that is labeled as a
            foreground.
        pos_iou_thresh (float): IoU threshold for a RoI to be considered as a
            foreground.
        neg_iou_thresh_hi (float): RoI is considered to be the background
            if IoU is in
            [:obj:`neg_iou_thresh_hi`, :obj:`neg_iou_thresh_hi`).
        neg_iou_thresh_lo (float): See above.
    """
    def __init__(self, n_sample=128, pos_ratio=0.25, pos_iou_thresh=0.5, 
        neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0):
        self.n_sample = n_sample
        self.pos_ratio = pos_ratio
        self.pos_iou_thresh = pos_iou_thresh
        self.neg_iou_thresh_hi = neg_iou_thresh_hi
        self.neg_iou_thresh_lo = neg_iou_thresh_lo

    def __call__(self, roi, bbox, label,
        loc_normalize_mean=(0., 0., 0., 0.), loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):
        """Assigns ground truth to sampled proposals.

        This function samples total of :obj:`self.n_sample` RoIs from the combination of 
        :obj:`roi` and :obj:`bbox`. The RoIs are assigned with the ground truth class labels 
        as well as bbox offsets and scales to match the ground truth bboxes. 
        As many as :obj:`pos_ratio * self.n_sample` RoIs are sampled as foregrounds.

        Offsets and scales of bboxes are calculated using
        :func:`model.utils.bbox_tools.bbox2loc`. 
        Also, types of input and output are same.

        Here are notations.

        * :math:`S` is the total number of sampled RoIs, which equals :obj:`self.n_sample`.
        * :math:`L` is number of object classes possibly including the background.

        Args:
            roi (tensor): Sampled RoIs. Its shape is :math:`(R, 4)`
            bbox (tensor): Coordinates of ground truth bboxes. Its shape is :math:`(R', 4)`.
            label (tensor): Ground truth bbox labels. Its shape is :math:`(R',)`. 
                Its range is :math:`[0, L - 1]`, where :math:`L` is the number of foreground classes.
            loc_normalize_mean (tuple of four floats): Mean values for normalizing bboxes.
            loc_normalize_std (tupler of four floats): Standard deviation for normalizing bboxes.

        Returns:
            (tensor, tensor, tensor):

            * **sample_roi**: Regions of interests that are sampled. Its shape is :math:`(S, 4)`.
            * **gt_roi_loc**: Offsets and scales to match the sampled RoIs to 
                the ground truth bboxes. Its shape is :math:`(S, 4)`.
            * **gt_roi_label**: Labels assigned to sampled RoIs. Its shape is :math:`(S,)`. 
                Its range is :math:`[0, L]`. The label with value 0 is the background.
        """
        # iou = bbox_iou(roi, bbox)
        # if (iou >= self.pos_iou_thresh).sum() == 0:
        roi = torch.cat([roi, bbox], dim=0)
        iou = bbox_iou(roi, bbox)
        roi_max_iou = iou.max(axis=1)[0]
        roi_gt_label = label[iou.argmax(axis=1)].long()

        # Positive
        # The label with value 0 is the background.
        n_pos_sample = int(self.n_sample * self.pos_ratio)
        pos_index = torch.where(roi_max_iou >= self.pos_iou_thresh)[0]
        if len(pos_index) >= n_pos_sample:
            pos_index = pos_index[
                np.random.choice(range(len(pos_index)), size=n_pos_sample, replace=False)]
        n_pos_sample = len(pos_index)
        # else:
        #     pos_app_index = pos_index[
        #         np.random.choice(range(len(pos_index)), size=n_pos_sample % len(pos_index), replace=False)]
        #     pos_index = torch.cat([pos_index] * (n_pos_sample // len(pos_index)) + [pos_app_index], dim=0)

        # Negative
        # Background RoIs from [neg_iou_thresh_lo, neg_iou_thresh_hi).
        n_neg_sample = int(self.n_sample - n_pos_sample)
        neg_index = torch.where((roi_max_iou < self.neg_iou_thresh_hi) & 
            (roi_max_iou >= self.neg_iou_thresh_lo))[0]
        # if len(neg_index) == 0:
        #     pass
        if len(neg_index) >= n_neg_sample:
            neg_index = neg_index[
                np.random.choice(range(len(neg_index)), size=n_neg_sample, replace=False)]
        # else:
        #     neg_app_index = neg_index[
        #         np.random.choice(range(len(neg_index)), size=n_neg_sample % len(neg_index), replace=False)]
        #     neg_index = torch.cat([neg_index] * (n_neg_sample // len(neg_index)) + [neg_app_index], dim=0)

        # The indices that we're selecting (both positive and negative).
        sample_index = torch.cat([pos_index, neg_index], dim=0)
        sample_roi_label = roi_gt_label[sample_index]
        sample_roi_label[n_pos_sample:] = 0
        sample_roi = roi[sample_index]

        # Compute offsets and scales to match sampled RoIs to the GTs.
        sample_roi_loc = bbox2loc(sample_roi, bbox[iou.argmax(axis=1)[sample_index]])
        loc_normalize_mean = torch.FloatTensor(loc_normalize_mean).to(roi.device)
        loc_normalize_std  = torch.FloatTensor(loc_normalize_std).to(roi.device)
        sample_roi_loc = (sample_roi_loc - loc_normalize_mean) / loc_normalize_std

        return sample_roi, sample_roi_loc, sample_roi_label


